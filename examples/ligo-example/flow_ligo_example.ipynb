{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea67bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import scipy.signal as ss\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import discovery.flow as dsf\n",
    "from flowjax.flows import triangular_spline_flow\n",
    "from flowjax.distributions import StandardNormal\n",
    "from ripplegw.waveforms.IMRPhenomD import gen_IMRPhenomD_hphc\n",
    "\n",
    "print(\"JAX devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d2a7c",
   "metadata": {},
   "source": [
    "## Set up LIGO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b12142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt=0.000244140625, fs=4096.0, df=0.05\n",
      "Freq points: 40961, in band: 9841\n",
      "<d|d> = 21160.8  (expected ~19680 for noise-only)\n",
      "noise-only log-lik = -10580.4\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 1: Load data with windowing + PSD\n",
    "# ==========================================\n",
    "from scipy.signal.windows import tukey\n",
    "\n",
    "data_raw = np.loadtxt(\"gw250114_20s.txt\")\n",
    "noise_raw = np.loadtxt(\"gw250114_50s_offsource.txt\")\n",
    "\n",
    "t, h = data_raw[:, 0], data_raw[:, 1]\n",
    "t_noise, h_noise = noise_raw[:, 0], noise_raw[:, 1]\n",
    "dt = float(t[1] - t[0])\n",
    "fs = 1.0 / dt\n",
    "fs_noise = 1/(float(t_noise[1] - t_noise[0]))\n",
    "\n",
    "# Apply Tukey window to suppress spectral leakage from low-freq seismic noise\n",
    "window = tukey(len(h), alpha=0.1)\n",
    "h_win = h * window\n",
    "\n",
    "f = jnp.fft.rfftfreq(len(h), d=dt)\n",
    "d_f = jnp.fft.rfft(h_win) * dt   # windowed FFT\n",
    "df = float(f[1] - f[0])\n",
    "\n",
    "# PSD from off-source noise (welch handles its own windowing internally)\n",
    "f_psd, psd_val = ss.welch(h_noise, fs=fs_noise, nperseg=int(fs_noise * 2))\n",
    "psd = jnp.interp(f, f_psd, psd_val)\n",
    "\n",
    "flow_freq, fhigh = 20.0, 512.0\n",
    "mask = (f >= flow_freq) & (f <= fhigh)\n",
    "\n",
    "# Sanity check: noise-only inner product should be ~2*T*BW = 2*20*492 ≈ 19680\n",
    "d_d = 4.0 * jnp.real(jnp.sum(jnp.abs(d_f[mask])**2 / psd[mask]) * df)\n",
    "print(f\"dt={dt}, fs={fs}, df={df}\")\n",
    "print(f\"Freq points: {len(f)}, in band: {mask.sum()}\")\n",
    "print(f\"<d|d> = {d_d:.1f}  (expected ~19680 for noise-only)\")\n",
    "print(f\"noise-only log-lik = {-0.5*d_d:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f375e3",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "Use ripplegw for the waveform generation because it returns an output differentiable in jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a356a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp max abs: 1.09e-19\n",
      "Log-likelihood at test point: -13788.61\n",
      "Log-lik ratio vs noise-only: -3208.22\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 2: Test waveform + likelihood with known-good params\n",
    "# ==========================================\n",
    "# ripplegw params: [Mc, eta, chi1, chi2, dist_mpc, tc, phic, iota]\n",
    "test_params = jnp.array([30.0, 0.25, 0.0, 0.0, 400.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "f_nz = f[1:]\n",
    "hp, hc = gen_IMRPhenomD_hphc(f_nz, test_params, flow_freq)\n",
    "\n",
    "h_det = jnp.concatenate([jnp.array([0j]), hp])\n",
    "h_det = jnp.where(f < flow_freq, 0j, h_det)\n",
    "res = d_f[mask] - h_det[mask]\n",
    "ll = -0.5 * 4.0 * jnp.real(jnp.sum(jnp.conj(res) * res / psd[mask]) * df)\n",
    "\n",
    "print(f\"hp max abs: {jnp.max(jnp.abs(hp)):.2e}\")\n",
    "print(f\"Log-likelihood at test point: {ll:.2f}\")\n",
    "print(f\"Log-lik ratio vs noise-only: {ll - (-0.5*d_d):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625a3fe",
   "metadata": {},
   "source": [
    "## Define LIGO Model that can be used as direct input into flow\n",
    "Since the discovery package is created for Pulsar Timing Array data, the Pulsar data classes are not applicable to LIGO data which contains strain, rather than correlations. However, we can still use the `discovery.flow` because it has been specified from the `Flowjax` built to do many things, among them variational inference.\n",
    "\n",
    "Variational inference allows us to take LIGO strain data and a physical waveform model as input, use a normalizing flow model to learn the shape of the posterior distribution that best explains the data, and optimize this approximation through gradient-based training.\n",
    "\n",
    "`discovery.flow` module has an elboLoss function which takes as input a target (parameters and ranges) and the number of samples (num params) which is used for the loss calculation. This loss is then fed as an input to the `dsf.variationalFit` trainer which the model uses to optimize the posterior distribution fit. Below, `LIGOModel` models the waveform, and `FlowTarget` is an object describing the parameters we're optimizing for.\n",
    "\n",
    "Regarding `_make_safe_target()` - I was running into errors because the loss was continuously returning nan values and the model was then not returning any physical values. From what I could see, the loss would be nan when the model returned unphysical values from the parameter space but then this nan would just be passed to each iteration so it never worked. Simply using set_nan_to_num (setting all nans = 0) also didn't work (according to claude, this is because of how the flow code works, if one param is nan they all become nan forever. I haven't dug deep into this, but see the explanation below). Adding in a specific function that ensures that nans for certain parameters don't poison the entire workflow seems to fix this. We get what look like physical parameter estimates. Time to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "g9jh5nsnk4l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target values and gradients on random z samples:\n",
      "  Mc=30.978, eta=0.167, chi1=0.474, chi2=0.093, dist=586.311, tc=-0.071, phic=5.751, iota=2.083\n",
      "    -> target=-1.11e+04, max|grad|=1.04e+03, any_nan=False\n",
      "  Mc=33.135, eta=0.165, chi1=0.392, chi2=-0.458, dist=708.327, tc=0.197, phic=3.936, iota=1.137\n",
      "    -> target=-1.09e+04, max|grad|=5.82e+02, any_nan=False\n",
      "  Mc=28.837, eta=0.218, chi1=-0.097, chi2=0.453, dist=576.541, tc=0.028, phic=3.727, iota=1.644\n",
      "    -> target=-1.09e+04, max|grad|=5.15e+02, any_nan=False\n",
      "  Mc=25.838, eta=0.190, chi1=0.403, chi2=-0.489, dist=320.544, tc=0.059, phic=0.570, iota=1.591\n",
      "    -> target=-1.15e+04, max|grad|=1.82e+03, any_nan=False\n",
      "  Mc=38.923, eta=0.151, chi1=-0.376, chi2=-0.444, dist=178.886, tc=0.152, phic=1.885, iota=2.148\n",
      "    -> target=-1.43e+04, max|grad|=6.98e+03, any_nan=False\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 3: Define model and target, test values + gradients\n",
    "# ==========================================\n",
    "class LIGOModel:\n",
    "    def __init__(self):\n",
    "        self.f, self.d_f, self.df = f, d_f, df\n",
    "        self.psd, self.mask = psd, mask\n",
    "        self.f_ref, self.f_min = flow_freq, flow_freq\n",
    "\n",
    "    def get_waveform(self, params):\n",
    "        f_nz = self.f[1:]\n",
    "        hp, hc = gen_IMRPhenomD_hphc(f_nz, params, self.f_ref)\n",
    "        h_det = jnp.concatenate([jnp.array([0j]), hp])\n",
    "        h_det = jnp.where(self.f < self.f_min, 0j, h_det)\n",
    "        # Replace NaN with 0 so bad parameter combos yield noise-only\n",
    "        # likelihood (finite value + finite gradient) instead of poisoning\n",
    "        # the entire batch gradient average\n",
    "        h_det = jnp.nan_to_num(h_det, nan=0.0)\n",
    "        return h_det\n",
    "\n",
    "    def log_likelihood(self, params):\n",
    "        h = self.get_waveform(params)\n",
    "        res = self.d_f[self.mask] - h[self.mask]\n",
    "        snr_sq = 4.0 * jnp.real(jnp.sum(jnp.conj(res) * res / self.psd[self.mask]) * self.df)\n",
    "        return -0.5 * snr_sq\n",
    "\n",
    "\n",
    "def _make_safe_target(log_lik_fn, map_fn, prior_fn):\n",
    "    \"\"\"Wrap a target function with NaN-safe gradients via custom_vjp.\n",
    "\n",
    "    The problem: ripplegw can produce NaN *internally* for certain parameter\n",
    "    combos. nan_to_num on the output fixes the forward value, but the backward\n",
    "    pass still propagates NaN gradients through ripplegw's internals.  Since\n",
    "    value_and_grad_ElboLoss averages gradients over all samples, a single NaN\n",
    "    gradient poisons the entire batch -> every subsequent step is NaN.\n",
    "\n",
    "    The fix: custom_vjp lets us intercept the backward pass and replace any\n",
    "    NaN gradient entries with 0.  A zero gradient means 'this sample contributes\n",
    "    nothing to the parameter update', which is the correct behaviour for a\n",
    "    sample that landed in an unphysical region.\n",
    "    \"\"\"\n",
    "    def _raw(z):\n",
    "        phys = map_fn(z)\n",
    "        return log_lik_fn(phys) + prior_fn(z)\n",
    "\n",
    "    @jax.custom_vjp\n",
    "    def _safe(z):\n",
    "        return jnp.nan_to_num(_raw(z), nan=-1e10, posinf=-1e10, neginf=-1e10)\n",
    "\n",
    "    def _fwd(z):\n",
    "        val = _safe(z)\n",
    "        return val, z\n",
    "\n",
    "    def _bwd(z, g):\n",
    "        grad = jax.grad(_raw)(z)\n",
    "        grad = jnp.nan_to_num(grad, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return (grad * g,)\n",
    "\n",
    "    _safe.defvjp(_fwd, _bwd)\n",
    "    return _safe\n",
    "\n",
    "\n",
    "class FlowTarget:\n",
    "    \"\"\"Target distribution for the normalizing flow.\n",
    "\n",
    "    Maps unconstrained z in R^8 to the 8 ripplegw IMRPhenomD parameters:\n",
    "        [Mc, eta, chi1, chi2, dist, tc, phic, iota]\n",
    "    via a tanh transform with specified bounds.\n",
    "    \"\"\"\n",
    "\n",
    "    param_names = ['Mc', 'eta', 'chi1', 'chi2', 'dist', 'tc', 'phic', 'iota']\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.dim = 8\n",
    "        # Bounds in ripplegw order: [Mc, eta, chi1, chi2, dist, tc, phic, iota]\n",
    "        # Tightened around GW250114 expected values for stable training\n",
    "        self.a = jnp.array([25.0,  0.15, -0.5, -0.5, 100.0, -0.2,   0.0,  0.0])\n",
    "        self.b = jnp.array([40.0,  0.25,  0.5,  0.5, 1000.0, 0.2, 2*jnp.pi, jnp.pi])\n",
    "        # Build NaN-safe callable (custom_vjp sanitises both value and gradient)\n",
    "        self._safe_fn = _make_safe_target(model.log_likelihood, self.map_to_physical, self.logprior)\n",
    "\n",
    "    def map_to_physical(self, z):\n",
    "        \"\"\"Map unconstrained z to bounded physical parameters via tanh.\"\"\"\n",
    "        return 0.5 * (self.b + self.a + (self.b - self.a) * jnp.tanh(z))\n",
    "\n",
    "    def samples_to_physical(self, z_batch):\n",
    "        \"\"\"Batch version of map_to_physical for post-training analysis.\"\"\"\n",
    "        return jax.vmap(self.map_to_physical)(z_batch)\n",
    "\n",
    "    def logprior(self, z):\n",
    "        \"\"\"Log-Jacobian of the tanh transform (uniform prior in physical space).\"\"\"\n",
    "        return jnp.sum(jnp.log(2.0) - 2.0 * jnp.logaddexp(z, -z))\n",
    "\n",
    "    def __call__(self, z):\n",
    "        return self._safe_fn(z)\n",
    "\n",
    "\n",
    "model = LIGOModel()\n",
    "target = FlowTarget(model)\n",
    "\n",
    "# Test values and gradients\n",
    "key = jax.random.key(0)\n",
    "z_test = jax.random.normal(key, (5, target.dim))\n",
    "grad_fn = jax.grad(target)\n",
    "\n",
    "# sanity check\n",
    "print(\"Target values and gradients on random z samples:\")\n",
    "for i in range(5):\n",
    "    z = z_test[i]\n",
    "    phys = target.map_to_physical(z)\n",
    "    val = target(z)\n",
    "    g = grad_fn(z)\n",
    "    names = target.param_names\n",
    "    param_str = \", \".join(f\"{names[j]}={phys[j]:.3f}\" for j in range(8))\n",
    "    print(f\"  {param_str}\")\n",
    "    print(f\"    -> target={val:.2e}, max|grad|={jnp.max(jnp.abs(g)):.2e}, any_nan={bool(jnp.any(jnp.isnan(g)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44875b1d",
   "metadata": {},
   "source": [
    "### Training & Run\n",
    "The initial loss is calculated using the discovery module ` value_and_grad_ElboLoss` taking the target from above as input. The model is trained using the discovery module `VariationalFit` taking the flow, loss, optimizer and a few other parameters as inputs. Annealing schedule is kept the same as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "v3qov11vnw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run (100 steps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.82it/s, loss=3765.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 3765.57\n",
      "Best loss:  307.09 at iter 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 4: Training run\n",
    "# ==========================================\n",
    "import optax\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "flow_key, train_key = jax.random.split(rng)\n",
    "\n",
    "flow = triangular_spline_flow(\n",
    "    flow_key,\n",
    "    base_dist=StandardNormal((target.dim,)),\n",
    "    flow_layers=12,\n",
    "    knots=10\n",
    ")\n",
    "\n",
    "# num_samples=16 runs in parallel on GPU, multibatch=4 loops sequentially\n",
    "# -> effective batch size of 64 but with more stable per-minibatch gradients\n",
    "loss_fn = dsf.value_and_grad_ElboLoss(target, num_samples=16)\n",
    "\n",
    "\n",
    "trainer = dsf.VariationalFit(\n",
    "    flow,\n",
    "    loss_fn=loss_fn,\n",
    "    multibatch=4,\n",
    "    learning_rate=1e-2,\n",
    "    annealing_schedule=lambda i: min(1.0, 0.01 + 0.99 * i / 500),\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"Training run (100 steps)...\")\n",
    "train_key, trained_flow = trainer.run(train_key, steps=100)\n",
    "print(f\"Final loss: {trainer.losses[-1]:.2f}\")\n",
    "print(f\"Best loss:  {min(trainer.losses):.2f} at iter {trainer.losses.index(min(trainer.losses))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21e2cc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Posterior summary (median [5th, 95th percentile])\n",
      "============================================================\n",
      "      Mc =   40.000  [  25.000,   40.000]\n",
      "     eta =    0.248  [   0.150,    0.250]\n",
      "      m1 =   46.756  [  28.717,  101.910]\n",
      "      m2 =   28.481  [  14.341,   45.948]\n",
      "       q =    0.853  [   0.225,    1.000]\n",
      "    chi1 =   -0.500  [  -0.500,    0.500]\n",
      "    chi2 =   -0.500  [  -0.500,    0.500]\n",
      "    dist = 1000.000  [ 100.000, 1000.000]\n",
      "      tc =   -0.200  [  -0.200,    0.200]\n",
      "    phic =    0.000  [   0.000,    6.283]\n",
      "    iota =    0.000  [   0.000,    3.142]\n",
      "\n",
      "Published GW250114 values for comparison:\n",
      "  Mc_det ~ 30.3-31.6 Msun\n",
      "  m1     ~ 32.6-34.9 Msun\n",
      "  m2     ~ 30.6-33.2 Msun\n",
      "  q      > 0.91\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 5: Extract results and visualise posteriors\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample from trained flow\n",
    "sample_key = jax.random.key(123)\n",
    "z_samples = trained_flow.sample(sample_key, sample_shape=(16384,))\n",
    "phys_samples = np.array(target.samples_to_physical(z_samples))\n",
    "\n",
    "# Build DataFrame with ripplegw parameter names\n",
    "post = pd.DataFrame(phys_samples, columns=target.param_names)\n",
    "\n",
    "# Derive component masses and mass ratio from Mc and eta\n",
    "#   M_total = Mc / eta^(3/5),  m2 = (M - sqrt(M^2 - 4*M^2*eta))/2,  m1 = M - m2\n",
    "Mc_s, eta_s = post['Mc'].values, post['eta'].values\n",
    "M_total = Mc_s / eta_s**0.6\n",
    "m2 = 0.5 * (M_total - np.sqrt(np.clip(M_total**2 * (1 - 4*eta_s), 0, None)))\n",
    "m1 = M_total - m2\n",
    "post['m1'] = m1\n",
    "post['m2'] = m2\n",
    "post['q'] = m2 / m1\n",
    "\n",
    "# Print summary statistics (median and 90% credible interval)\n",
    "print(\"=\" * 60)\n",
    "print(\"Posterior summary (median [5th, 95th percentile])\")\n",
    "print(\"=\" * 60)\n",
    "for col in ['Mc', 'eta', 'm1', 'm2', 'q', 'chi1', 'chi2', 'dist', 'tc', 'phic', 'iota']:\n",
    "    lo, med, hi = np.percentile(post[col], [5, 50, 95])\n",
    "    print(f\"  {col:>6s} = {med:8.3f}  [{lo:8.3f}, {hi:8.3f}]\")\n",
    "\n",
    "print(\"\\nPublished GW250114 values for comparison:\")\n",
    "print(\"  Mc_det ~ 30.3-31.6 Msun\")\n",
    "print(\"  m1     ~ 32.6-34.9 Msun\")\n",
    "print(\"  m2     ~ 30.6-33.2 Msun\")\n",
    "print(\"  q      > 0.91\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a86dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "801efe93",
   "metadata": {},
   "source": [
    "# Appendix : Claude's explanation of the nan problem\n",
    "\n",
    "The NaN problem explained\n",
    "The value_and_grad_ElboLoss in flow.py:43 does:\n",
    "\n",
    "```\n",
    "vals, grad = eqx.filter_vmap(eqx.filter_value_and_grad(theloss), ...)(...)\n",
    "return vals.mean(), jax.tree_util.tree_map(lambda array: jnp.mean(array, axis=0), grad)\n",
    "```\n",
    "\n",
    "It computes 16 independent (value, gradient) pairs and averages them. If even one sample produces a NaN gradient, jnp.mean over gradients = NaN → the optimizer applies a NaN update → all flow parameters become NaN → every subsequent step is NaN forever.\n",
    "\n",
    "The old nan_to_num on the forward output didn't help because jnp.nan_to_num has zero gradient at NaN points — it doesn't intercept the backward pass. When ripplegw produces NaN internally during intermediate computations, the gradient through those intermediate values is NaN even if the final output was clamped.\n",
    "\n",
    "The fix: jax.custom_vjp\n",
    "The new _make_safe_target wrapper uses jax.custom_vjp to intercept both the forward and backward pass:\n",
    "\n",
    "Forward (_fwd): computes the target value, replaces NaN with -1e10 (penalty)\n",
    "Backward (_bwd): computes the gradient normally via jax.grad(_raw), then applies nan_to_num to replace any NaN gradient entries with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981ca61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jax-gw)",
   "language": "python",
   "name": "jax-gw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
